{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8233aad0-5b7f-4150-8b2b-af77cf626aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 04:17:51 WARN Utils: Your hostname, codespaces-be4276 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/03/01 04:17:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/01 04:17:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 04:18:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Module 5 Zoomcamp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee47b55",
   "metadata": {},
   "source": [
    "#### Question 1. Spark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef92512c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28230ef",
   "metadata": {},
   "source": [
    "#### Question 2. FHV October 2019 partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77b9ba6-3fc6-4bed-8033-cbd5b378db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CSV to Spark DataFrame\n",
    "csv_file = \"data/fhv_tripdata_2019-10.csv\"\n",
    "df = spark.read.csv(csv_file, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "597f5931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Repartition DataFrame to 6 Partitions\n",
    "df_rpt = df.repartition(6)\n",
    "\n",
    "# DataFrame to Parquet\n",
    "parquet_output = \"data/output\"\n",
    "df_rpt.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(parquet_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3541c51",
   "metadata": {},
   "source": [
    "#### Question 3. Count records on 15th of October"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23ac4f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PUlocationID', IntegerType(), True), StructField('DOlocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84a40c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8559207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_rows = df.filter(col(\"pickup_datetime\").cast(\"date\") == '2019-10-15').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fa420c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1f461",
   "metadata": {},
   "source": [
    "#### Question 4. The longest trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06a1505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, hour, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "620947c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "631152.5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"pickup_timestamp\", unix_timestamp(col(\"pickup_datetime\")).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"dropoff_timestamp\", unix_timestamp(col(\"dropoff_datetime\")).cast(\"timestamp\"))\n",
    "    \n",
    "df = df.withColumn(\"duration\", (col(\"dropoff_timestamp\").cast(\"long\") - col(\"pickup_timestamp\").cast(\"long\")) / 3600)\n",
    "max_duration = df.agg({\"duration\": \"max\"}).collect()[0][0]\n",
    "max_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c634ff",
   "metadata": {},
   "source": [
    "#### Question 5. Spark UI port\n",
    "\n",
    "4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337ae06",
   "metadata": {},
   "source": [
    "#### Question 6. Least frequent pickup location zone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a642041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV data into DataFrame\n",
    "csv_file_path = \"data/taxi_zone_lookup.csv\"\n",
    "df_zone = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86129c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dispatching_base_num='B00009', pickup_datetime=datetime.datetime(2019, 10, 1, 0, 23), dropOff_datetime=datetime.datetime(2019, 10, 1, 0, 35), PUlocationID=264, DOlocationID=264, SR_Flag=None, Affiliated_base_number='B00009', pickup_timestamp=datetime.datetime(2019, 10, 1, 0, 23), dropoff_timestamp=datetime.datetime(2019, 10, 1, 0, 35), duration=0.2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d541a1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(LocationID=1, Borough='EWR', Zone='Newark Airport', service_zone='EWR')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d7ef030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "|        Battery Park|   15|\n",
      "|Saint Michaels Ce...|   23|\n",
      "|Breezy Point/Fort...|   25|\n",
      "|Marine Park/Floyd...|   26|\n",
      "|        Astoria Park|   29|\n",
      "|    Inwood Hill Park|   39|\n",
      "|       Willets Point|   47|\n",
      "|Forest Park/Highl...|   53|\n",
      "|  Brooklyn Navy Yard|   57|\n",
      "|        Crotona Park|   62|\n",
      "|        Country Club|   77|\n",
      "|     Freshkills Park|   89|\n",
      "|       Prospect Park|   98|\n",
      "|     Columbia Street|  105|\n",
      "|  South Williamsburg|  110|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "joined_df = df.join(df_zone, df.PUlocationID == df_zone.LocationID, \"left\")\n",
    "\n",
    "result_df = joined_df.groupBy(\"Zone\").count()\n",
    "result_df = result_df.orderBy(\"count\")\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eeed63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
